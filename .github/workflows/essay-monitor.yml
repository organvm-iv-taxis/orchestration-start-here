name: Essay Monitor

# Watches ORGAN-V public-process for new essays and surfaces them
# for distribution coordination. Creates an issue per new essay
# with the ready-to-distribute label to trigger distribute-content.yml.

on:
  schedule:
    # Daily at 09:00 UTC
    - cron: '0 9 * * *'
  workflow_dispatch:

jobs:
  monitor:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      issues: write

    steps:
      - name: Checkout orchestration hub
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Check for new essays
        env:
          GH_TOKEN: ${{ secrets.CROSS_ORG_TOKEN || secrets.GITHUB_TOKEN }}
        run: |
          python3 << 'ESSAY_MONITOR'
          import json
          import os
          import re
          import subprocess
          import sys
          from datetime import datetime, timedelta, timezone

          def gh_api(endpoint):
              result = subprocess.run(
                  ['gh', 'api', endpoint],
                  capture_output=True, text=True
              )
              if result.returncode != 0:
                  return None
              try:
                  return json.loads(result.stdout)
              except json.JSONDecodeError:
                  return None

          def gh_api_raw(endpoint):
              """Fetch raw content from GitHub API."""
              result = subprocess.run(
                  ['gh', 'api', endpoint, '-H', 'Accept: application/vnd.github.raw+json'],
                  capture_output=True, text=True
              )
              if result.returncode != 0:
                  return None
              return result.stdout

          def parse_frontmatter(content):
              """Extract YAML frontmatter from Jekyll markdown."""
              match = re.match(r'^---\s*\n(.*?)\n---', content, re.DOTALL)
              if not match:
                  return {}
              fm = {}
              for line in match.group(1).split('\n'):
                  line = line.strip()
                  if ':' not in line or line.startswith('#'):
                      continue
                  key, _, value = line.partition(':')
                  key = key.strip()
                  value = value.strip().strip('"').strip("'")
                  if value.startswith('[') and value.endswith(']'):
                      value = [v.strip().strip('"').strip("'") for v in value[1:-1].split(',')]
                  fm[key] = value
              return fm

          def get_existing_essay_issues():
              """Get titles of existing essay-monitor issues to avoid duplicates."""
              titles = set()
              page = 1
              while True:
                  issues = gh_api(
                      f'repos/organvm-iv-taxis/orchestration-start-here/issues'
                      f'?labels=essay-detected&state=all&per_page=100&page={page}'
                  )
                  if not issues:
                      break
                  for i in issues:
                      titles.add(i.get('title', ''))
                  if len(issues) < 100:
                      break
                  page += 1
              return titles

          def create_issue(title, body, labels):
              """Create a GitHub issue."""
              cmd = ['gh', 'issue', 'create', '--title', title, '--body', body]
              for label in labels:
                  cmd.extend(['--label', label])
              result = subprocess.run(cmd, capture_output=True, text=True)
              return result.returncode == 0, result.stdout.strip()

          # Fetch _posts directory listing from ORGAN-V
          posts = gh_api('repos/organvm-v-logos/public-process/contents/_posts')
          if not posts:
              print("Could not fetch _posts from public-process. Using GITHUB_TOKEN may lack cross-org access.")
              print("Set CROSS_ORG_TOKEN secret for cross-org monitoring.")
              sys.exit(0)

          # Parse essay metadata — only essays from the last 7 days
          now = datetime.now(timezone.utc).replace(tzinfo=None)
          lookback_days = 7
          cutoff = now - timedelta(days=lookback_days)

          essays = []
          for post in posts:
              name = post.get('name', '')
              if not name.endswith('.md'):
                  continue
              parts = name.split('-', 3)
              if len(parts) < 4:
                  continue
              try:
                  post_date = datetime(int(parts[0]), int(parts[1]), int(parts[2]))
              except (ValueError, IndexError):
                  continue

              if post_date >= cutoff:
                  slug = parts[3].replace('.md', '')
                  essays.append({
                      'filename': name,
                      'date': post_date.strftime('%Y-%m-%d'),
                      'slug': slug,
                      'api_path': post.get('path', f'_posts/{name}'),
                      'github_url': post.get('html_url', ''),
                  })

          if not essays:
              print(f"No new essays in the last {lookback_days} days.")
              sys.exit(0)

          # Check which essays we've already surfaced
          existing = get_existing_essay_issues()

          # Ensure labels exist
          for label_name in ['essay-detected', 'ready-to-distribute']:
              subprocess.run(
                  ['gh', 'label', 'create', label_name, '--color', '0E8A16',
                   '--description', f'Auto-created by essay-monitor'],
                  capture_output=True, text=True
              )

          # Create issues for new essays (with frontmatter enrichment)
          created = 0
          for essay in essays:
              # Fetch raw content for frontmatter
              raw = gh_api_raw(f"repos/organvm-v-logos/public-process/contents/{essay['api_path']}")
              fm = parse_frontmatter(raw) if raw else {}

              title = fm.get('title', essay['slug'].replace('-', ' ').title())
              issue_title = f"Essay Detected: {title}"

              if issue_title in existing:
                  continue

              excerpt = fm.get('excerpt', fm.get('description', ''))
              tags = fm.get('tags', [])
              if isinstance(tags, str):
                  tags = [t.strip() for t in tags.split(',')]
              category = fm.get('category', fm.get('categories', ''))
              reading_time = fm.get('reading_time', '')
              essay_url = (
                  f"https://organvm-v-logos.github.io/public-process/"
                  f"{essay['date'].replace('-', '/')}/{essay['slug']}/"
              )

              tags_str = ', '.join(tags) if isinstance(tags, list) else str(tags)
              body_lines = [
                  "## New Essay Published",
                  "",
                  f"**Title:** {title}",
                  f"**Date:** {essay['date']}",
                  f"**URL:** {essay_url}",
                  f"**Excerpt:** {excerpt}" if excerpt else None,
                  f"**Tags:** {tags_str}" if tags_str else None,
                  f"**Category:** {category}" if category else None,
                  f"**Reading Time:** {reading_time}" if reading_time else None,
                  f"**Source:** [{essay['filename']}]({essay['github_url']})",
                  "",
                  "### Distribution Status",
                  "",
                  "Auto-distributed via POSSE pipeline.",
                  "The `ready-to-distribute` label triggers `distribute-content.yml` automatically.",
                  "",
                  "---",
                  "*Generated by essay-monitor.yml (DISTRIBUTIO Sprint)*",
              ]
              body = "\n".join(line for line in body_lines if line is not None)

              ok, url = create_issue(issue_title, body, ['essay-detected', 'ready-to-distribute'])
              if ok:
                  print(f"  Created: {issue_title} → {url}")
                  created += 1
              else:
                  print(f"  Failed: {issue_title}")

          print(f"\nProcessed {len(essays)} recent essays, created {created} new issues.")

          ESSAY_MONITOR
